{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26068ef8-a94e-45fe-a941-a3a726ed6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor, ClapModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77ade16a-e612-4fb0-8956-95903f7bb49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. –ö–æ–Ω—Ñ–∏–≥\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_json: str = \"final_dataset/train.json\"\n",
    "    val_json: str   = \"final_dataset/val.json\"\n",
    "    test_json: str  = \"final_dataset/test.json\"\n",
    "\n",
    "    text_model_name: str = \"BAAI/bge-m3\"\n",
    "    projection_dim: int = 2048\n",
    "    dropout: float = 0.3\n",
    "    biencoder_ckpt: str = \"25_ep_tag_hard_negs_bge_8192/bi_encoder_best.pth\"\n",
    "\n",
    "    clap_ckpt_dir: str = \"clap\"  # dir —Å save_pretrained() –∏–∑ Jamendo‚Äë—Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "    audio_sr: int = 48000\n",
    "    max_audio_seconds: int = 30\n",
    "\n",
    "    max_desc_len: int = 4096\n",
    "    max_lyrics_len: int = 4096\n",
    "\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 0\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-5\n",
    "    weight_decay: float = 1e-2\n",
    "    fused_dim: int = 256\n",
    "    temperature: float = 0.07\n",
    "\n",
    "    out_dir: str = \"fusion_ckpts\"\n",
    "    val_log_path: str = \"fusion_val_losses.json\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "AUDIO_ROOT = os.path.expanduser('~/persistent_volume/final_dataset/audio')  # –∏–ª–∏ –ø—É—Ç—å –≤ –∫–æ–ª–∞–±–µ/–∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "PROJECT_DIR = os.path.expanduser('~/persistent_volume')\n",
    "OUT_TRAIN_FEATS = os.path.join(PROJECT_DIR, 'final_dataset', 'train_features.pt')\n",
    "OUT_VAL_FEATS = os.path.join(PROJECT_DIR, 'final_dataset', 'val_features.pt')\n",
    "OUT_TEST_FEATS = os.path.join(PROJECT_DIR, 'final_dataset', 'test_features.pt')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13923b05-fd68-4119-aefa-1b2a7d69c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abd5b550-87a3-4a82-a2bf-3447f2a9295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in /home/jovyan/persistent_volume/final_dataset/test.json: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features:   8%|‚ñä         | 336/4096 [06:48<1:15:42,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipping sKKgabkwl44SoTR7: failed to load audio (Failed to open the input \"/home/jovyan/persistent_volume/final_dataset/audio/sKKgabkwl44SoTR7.mp3\" (Invalid argument).)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2448/4096 [49:31<34:43,  1.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipping z3m1XXTGReQx4Jdd: failed to load audio (Failed to open the input \"/home/jovyan/persistent_volume/final_dataset/audio/z3m1XXTGReQx4Jdd.mp3\" (Invalid argument).)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2592/4096 [52:24<31:43,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipping wRRgs5XpbKKIe2KZ: failed to load audio (Failed to open the input \"/home/jovyan/persistent_volume/final_dataset/audio/wRRgs5XpbKKIe2KZ.mp3\" (Invalid argument).)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4096/4096 [1:24:08<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed features for 1363 tracks (from 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor, ClapModel\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === –ö–æ–Ω—Ñ–∏–≥ –∏ –ø—É—Ç–∏ ===\n",
    "\n",
    "PROJECT_DIR = os.path.expanduser('~/persistent_volume')\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'final_dataset')\n",
    "AUDIO_ROOT = os.path.join(DATA_DIR, 'audio')\n",
    "\n",
    "TEXT_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "BIENCODER_CKPT = \"25_ep_tag_hard_negs_bge_8192/bi_encoder_best.pth\"\n",
    "CLAP_CKPT_DIR = \"clap\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === BiEncoder ===\n",
    "\n",
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, backbone, projection_dim, p_drop):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        emb_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        def head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(emb_dim, projection_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p_drop),\n",
    "                nn.Linear(projection_dim, projection_dim),\n",
    "                nn.Dropout(p_drop),\n",
    "            )\n",
    "\n",
    "        self.desc_head = head()\n",
    "        self.lyr_head = head()\n",
    "\n",
    "    def _mean_pool(self, outputs, attention_mask):\n",
    "        hs = outputs.last_hidden_state.float()\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        denom = mask.sum(1).clamp_min(1e-6)\n",
    "        pooled = (hs * mask).sum(1) / denom\n",
    "        return pooled\n",
    "\n",
    "    def encode_description(self, ids, mask):\n",
    "        out = self.backbone(input_ids=ids, attention_mask=mask)\n",
    "        proj = self.desc_head(self._mean_pool(out, mask))\n",
    "        return F.normalize(proj, p=2, dim=1)\n",
    "\n",
    "    def encode_lyrics(self, ids, mask):\n",
    "        out = self.backbone(input_ids=ids, attention_mask=mask)\n",
    "        proj = self.lyr_head(self._mean_pool(out, mask))\n",
    "        return F.normalize(proj, p=2, dim=1)\n",
    "\n",
    "print(\"Loading BiEncoder & CLAP...\")\n",
    "\n",
    "text_tok = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "text_backbone = AutoModel.from_pretrained(TEXT_MODEL_NAME, trust_remote_code=True).to(device).eval()\n",
    "biencoder = BiEncoder(text_backbone, projection_dim=2048, p_drop=0.3).to(device)\n",
    "biencoder.load_state_dict(torch.load(BIENCODER_CKPT, map_location='cpu'))\n",
    "biencoder.eval()\n",
    "\n",
    "clap_processor = AutoProcessor.from_pretrained(CLAP_CKPT_DIR)\n",
    "clap_model = ClapModel.from_pretrained(CLAP_CKPT_DIR).to(device).eval()\n",
    "clap_sr = clap_processor.feature_extractor.sampling_rate\n",
    "\n",
    "print(\"‚úì Models loaded\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_split(split_name: str):\n",
    "    json_path = os.path.join(DATA_DIR, f\"{split_name}.json\")\n",
    "    out_feats_path = os.path.join(DATA_DIR, f\"{split_name}_features.pt\")\n",
    "\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"\\n--- Processing split: {split_name} ---\")\n",
    "    print(f\"Items in {json_path}: {len(data)}\")\n",
    "\n",
    "    all_feats = {}\n",
    "\n",
    "    def process_batch(batch_items):\n",
    "        nonlocal all_feats\n",
    "\n",
    "        wavs = []\n",
    "        tids = []\n",
    "        lyrics_list = []\n",
    "        desc_list = []\n",
    "\n",
    "        # 1) —á–∏—Ç–∞–µ–º –∞—É–¥–∏–æ –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤\n",
    "        for item in batch_items:\n",
    "            tid = item['track_id']\n",
    "            path = os.path.join(AUDIO_ROOT, f\"{tid}.mp3\")\n",
    "            try:\n",
    "                wav, sr = torchaudio.load(path)\n",
    "            except Exception as e:\n",
    "                print(f\"  Skipping {tid}: failed to load audio ({e})\")\n",
    "                continue\n",
    "\n",
    "            if sr != clap_sr:\n",
    "                wav = torchaudio.functional.resample(wav, sr, clap_sr)\n",
    "            if wav.size(0) > 1:\n",
    "                wav = wav.mean(0, keepdim=True)\n",
    "\n",
    "            wavs.append(wav.squeeze(0).cpu().numpy())\n",
    "            tids.append(tid)\n",
    "\n",
    "            lyr = item.get('lyrics', \"\")\n",
    "            desc = item.get('description', \"\")\n",
    "            if not isinstance(lyr, str):\n",
    "                lyr = \"\"\n",
    "            if not isinstance(desc, str):\n",
    "                desc = \"\"\n",
    "            lyrics_list.append(lyr)\n",
    "            desc_list.append(desc)\n",
    "\n",
    "        if not wavs:\n",
    "            return\n",
    "\n",
    "        # 2) CLAP audio\n",
    "        audio_inputs = clap_processor.feature_extractor(\n",
    "            raw_speech=wavs,\n",
    "            sampling_rate=clap_sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        audio_inputs = {k: v.to(device) for k, v in audio_inputs.items()}\n",
    "        audio_emb = clap_model.get_audio_features(\n",
    "            input_features=audio_inputs['input_features']\n",
    "        )\n",
    "\n",
    "        # 3) —Ç–µ–∫—Å—Ç—ã –±–∞—Ç—á–µ–º (BiEncoder + CLAP text)\n",
    "        try:\n",
    "            lyr_enc = text_tok(\n",
    "                lyrics_list, truncation=True, padding=True,\n",
    "                max_length=1024, return_tensors='pt'\n",
    "            ).to(device)\n",
    "            desc_enc = text_tok(\n",
    "                desc_list, truncation=True, padding=True,\n",
    "                max_length=512, return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            lyr_emb = biencoder.encode_lyrics(\n",
    "                lyr_enc['input_ids'], lyr_enc['attention_mask']\n",
    "            )\n",
    "            desc_txt_emb = biencoder.encode_description(\n",
    "                desc_enc['input_ids'], desc_enc['attention_mask']\n",
    "            )\n",
    "\n",
    "            clap_txt = clap_processor(\n",
    "                text=desc_list, return_tensors='pt', padding=True, truncation=True\n",
    "            )\n",
    "            clap_txt = {k: v.to(device) for k, v in clap_txt.items()}\n",
    "            desc_audio_emb = clap_model.get_text_features(\n",
    "                input_ids=clap_txt['input_ids'],\n",
    "                attention_mask=clap_txt['attention_mask'],\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e) or \"CUDACachingAllocator\" in str(e):\n",
    "                print(\"  Skipping batch due to OOM:\", e)\n",
    "                torch.cuda.empty_cache()\n",
    "                return\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        B = len(tids)\n",
    "        assert audio_emb.size(0) == B\n",
    "        assert lyr_emb.size(0) == B\n",
    "        assert desc_txt_emb.size(0) == B\n",
    "        assert desc_audio_emb.size(0) == B\n",
    "\n",
    "        for i, tid in enumerate(tids):\n",
    "            all_feats[tid] = {\n",
    "                'audio_emb':      audio_emb[i].cpu(),\n",
    "                'lyrics_emb':     lyr_emb[i].cpu(),\n",
    "                'desc_audio_emb': desc_audio_emb[i].cpu(),\n",
    "                'desc_text_emb':  desc_txt_emb[i].cpu(),\n",
    "            }\n",
    "\n",
    "    batch = []\n",
    "    for item in tqdm(data, desc=f\"Precomputing {split_name}\"):\n",
    "        batch.append({\n",
    "            'track_id':    item['track_id'],\n",
    "            'lyrics':      item.get('lyrics', \"\"),\n",
    "            'description': item.get('description', \"\"),\n",
    "        })\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            process_batch(batch)\n",
    "            batch = []\n",
    "\n",
    "    if batch:\n",
    "        process_batch(batch)\n",
    "\n",
    "    print(f\"Computed features for {len(all_feats)} tracks (from {len(data)})\")\n",
    "    torch.save(all_feats, out_feats_path)\n",
    "    print(f\"Saved features to {out_feats_path}\")\n",
    "\n",
    "# –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ –æ—á–µ—Ä–µ–¥–∏\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    process_split(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f9d757d-5608-4d1d-b54c-dd7d5561b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. BiEncoder\n",
    "# ============================================================\n",
    "\n",
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, backbone, projection_dim, p_drop):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        emb_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        def head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(emb_dim, projection_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p_drop),\n",
    "                nn.Linear(projection_dim, projection_dim),\n",
    "                nn.Dropout(p_drop),\n",
    "            )\n",
    "\n",
    "        self.desc_head = head()\n",
    "        self.lyr_head = head()\n",
    "\n",
    "    def _mean_pool(self, outputs, attention_mask):\n",
    "        # –∫–∞–∫ —É —Ç–µ–±—è: –±–µ–∑ autocast, –≤ float32 [file:22]\n",
    "        hs = outputs.last_hidden_state.float()\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        denom = mask.sum(1).clamp_min(1e-6)\n",
    "        pooled = (hs * mask).sum(1) / denom\n",
    "        return pooled\n",
    "\n",
    "    def encode_description(self, ids, mask):\n",
    "        out = self.backbone(input_ids=ids, attention_mask=mask)\n",
    "        proj = self.desc_head(self._mean_pool(out, mask))\n",
    "        return F.normalize(proj, p=2, dim=1)\n",
    "\n",
    "    def encode_lyrics(self, ids, mask):\n",
    "        out = self.backbone(input_ids=ids, attention_mask=mask)\n",
    "        proj = self.lyr_head(self._mean_pool(out, mask))\n",
    "        return F.normalize(proj, p=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf858657-b299-40a0-a343-1ff734757cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BiEncoder backbone/tokenizer...\n",
      "‚úì BiEncoder loaded from 25_ep_tag_hard_negs_bge_8192/bi_encoder_best.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading BiEncoder backbone/tokenizer...\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "text_backbone = AutoModel.from_pretrained(cfg.text_model_name, trust_remote_code=True)\n",
    "text_backbone.gradient_checkpointing_enable()\n",
    "biencoder = BiEncoder(text_backbone, cfg.projection_dim, cfg.dropout).to(device)\n",
    "\n",
    "# –≥—Ä—É–∑–∏–º —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤—ã, backbone –∑–∞–º–æ—Ä–æ–∑–∏–º\n",
    "state = torch.load(cfg.biencoder_ckpt, map_location=\"cpu\")\n",
    "biencoder.load_state_dict(state)\n",
    "for p in biencoder.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "biencoder.eval()\n",
    "print(\"‚úì BiEncoder loaded from\", cfg.biencoder_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfd3e0f7-e937-46e6-a43f-3c8b547e9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLAP model & processor...\n",
      "‚úì CLAP loaded from clap\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. CLAP\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading CLAP model & processor...\")\n",
    "clap_processor = AutoProcessor.from_pretrained(cfg.clap_ckpt_dir)\n",
    "clap_model = ClapModel.from_pretrained(cfg.clap_ckpt_dir).to(device)\n",
    "clap_model.eval()\n",
    "for p in clap_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"‚úì CLAP loaded from\", cfg.clap_ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6af0f2f7-f2c4-47b8-9417-0416170c4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Fusion encoder (concat + MLP)\n",
    "# ============================================================\n",
    "\n",
    "D_AUDIO = clap_model.config.projection_dim  # –æ–±—ã—á–Ω–æ 512 [file:139]\n",
    "D_TEXT = cfg.projection_dim                # 2048 –∏–∑ BiEncoder\n",
    "\n",
    "class FusionEncoder(nn.Module):\n",
    "    def __init__(self, dim_audio, dim_text, fused_dim=512, hidden=1024, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        in_dim = dim_audio + dim_text\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, fused_dim),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(fused_dim)\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        x = torch.cat([audio_emb, text_emb], dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        x = self.norm(x)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "fusion_encoder = FusionEncoder(D_AUDIO, D_TEXT,\n",
    "                               fused_dim=256,\n",
    "                               hidden=1024,\n",
    "                               p_drop=0.2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "acfd3a8f-7825-4af8-94b2-8e97c11d9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Dataset: audio + lyrics + description\n",
    "# ============================================================\n",
    "\n",
    "import os, json, torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, json_path, feats_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            items = json.load(f)\n",
    "        feats = torch.load(feats_path)\n",
    "\n",
    "        self.pairs = []\n",
    "        for it in items:\n",
    "            tid = it[\"track_id\"]\n",
    "            if tid not in feats:\n",
    "                continue  # —Ç—Ä–µ–∫–∏ –±–µ–∑ —Ñ–∏—á –≤—ã–∫–∏–¥—ã–≤–∞–µ–º\n",
    "            self.pairs.append({\n",
    "                \"track_id\": tid,\n",
    "                \"description\": it[\"description\"],\n",
    "            })\n",
    "        self.feats = feats\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        tid = pair[\"track_id\"]\n",
    "        f = self.feats[tid]\n",
    "        return {\n",
    "            \"track_id\":      tid,\n",
    "            \"audio_emb\":      f[\"audio_emb\"],      # —Ç—Ä–µ–∫–æ–≤—ã–µ —Ñ–∏—á–∏\n",
    "            \"lyrics_emb\":     f[\"lyrics_emb\"],\n",
    "            \"desc_audio_emb\": f[\"desc_audio_emb\"], # CLAP‚Äë—Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è\n",
    "            \"desc_text_emb\":  f[\"desc_text_emb\"],  # BiEncoder‚Äë—Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"track_id\":      [b[\"track_id\"] for b in batch],\n",
    "        \"audio_emb\":      torch.stack([b[\"audio_emb\"] for b in batch]),\n",
    "        \"lyrics_emb\":     torch.stack([b[\"lyrics_emb\"] for b in batch]),\n",
    "        \"desc_audio_emb\": torch.stack([b[\"desc_audio_emb\"] for b in batch]),\n",
    "        \"desc_text_emb\":  torch.stack([b[\"desc_text_emb\"] for b in batch]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4032a4d8-f9e5-4678-b63d-6445ea02fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 32749, Val: 4093, Test: 4093\n"
     ]
    }
   ],
   "source": [
    "train_ds = FusionDataset(cfg.train_json, feats_path=OUT_TRAIN_FEATS)\n",
    "val_ds   = FusionDataset(cfg.val_json, feats_path=OUT_VAL_FEATS)\n",
    "test_ds  = FusionDataset(cfg.test_json, feats_path=OUT_TEST_FEATS)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c9c2c147-25c8-498d-bc5b-114f13e2b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Loss –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "# ============================================================\n",
    "\n",
    "def clip_loss(q, t, temperature):\n",
    "    logits = (q @ t.t()) / temperature\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "\n",
    "optimizer = AdamW(fusion_encoder.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=cfg.lr,\n",
    "                       epochs=cfg.epochs, steps_per_epoch=len(train_loader))\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35c26e5e-00ef-47e3-88e2-46a447663fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "best_path = os.path.join(cfg.out_dir, \"fusion_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d94faf7-c4c4-44c0-994d-c2497e4965ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: encode –∏ metrics\n",
    "# ============================================================\n",
    "\n",
    "def encode_batch(batch):\n",
    "    audio = batch[\"audio\"].to(device)\n",
    "    lyr_ids = batch[\"lyrics_input_ids\"].to(device)\n",
    "    lyr_mask = batch[\"lyrics_attention_mask\"].to(device)\n",
    "    desc_ids = batch[\"desc_input_ids\"].to(device)\n",
    "    desc_mask = batch[\"desc_attention_mask\"].to(device)\n",
    "    clap_ids = batch[\"clap_input_ids\"].to(device)\n",
    "    clap_mask = batch[\"clap_attention_mask\"]\n",
    "    if clap_mask is not None:\n",
    "        clap_mask = clap_mask.to(device)\n",
    "\n",
    "    # –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã —Å—á–∏—Ç–∞–µ–º –ø–æ–¥ no_grad\n",
    "    with torch.no_grad():\n",
    "        audio_inputs = clap_processor.feature_extractor(\n",
    "            raw_speech=audio.squeeze(1).cpu().numpy(),\n",
    "            sampling_rate=clap_processor.feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        audio_feat = clap_model.get_audio_features(\n",
    "            input_features=audio_inputs[\"input_features\"].to(device)\n",
    "        )                           # (B, D_A)\n",
    "        audio_feat = F.normalize(audio_feat, p=2, dim=-1)\n",
    "\n",
    "        lyr_emb = biencoder.encode_lyrics(lyr_ids, lyr_mask)          # (B, D_T)\n",
    "        desc_emb = biencoder.encode_description(desc_ids, desc_mask)  # (B, D_T)\n",
    "\n",
    "        clap_text_feat = clap_model.get_text_features(\n",
    "            input_ids=clap_ids,\n",
    "            attention_mask=clap_mask,\n",
    "        )                           # (B, D_A)\n",
    "        clap_text_feat = F.normalize(clap_text_feat, p=2, dim=-1)\n",
    "\n",
    "    # fusion_encoder –±–µ–∑ no_grad\n",
    "    track_fused = fusion_encoder(audio_feat, lyr_emb)        # (B, D_fused)\n",
    "    query_fused = fusion_encoder(clap_text_feat, desc_emb)   # (B, D_fused)\n",
    "\n",
    "    return query_fused, track_fused\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_val():\n",
    "    fusion_encoder.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        q = fusion_encoder(batch['desc_audio_emb'].to(device),\n",
    "                           batch['desc_text_emb'].to(device))\n",
    "        t = fusion_encoder(batch['audio_emb'].to(device),\n",
    "                           batch['lyrics_emb'].to(device))\n",
    "        loss = clip_loss(q, t, cfg.temperature)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(loader, model):\n",
    "    model.eval()\n",
    "    all_q, all_t = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Computing embeddings\"):\n",
    "        q = model(batch['desc_audio_emb'].to(device),\n",
    "                           batch['desc_text_emb'].to(device))\n",
    "        t = model(batch['audio_emb'].to(device),\n",
    "                           batch['lyrics_emb'].to(device))\n",
    "        all_q.append(q.cpu())\n",
    "        all_t.append(t.cpu())\n",
    "\n",
    "    q_emb = torch.cat(all_q, dim=0)  # (N, D)\n",
    "    t_emb = torch.cat(all_t, dim=0)  # (N, D)\n",
    "    return q_emb, t_emb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_retrieval_metrics(q_emb, t_emb, k_values=(1, 5, 10, 20)):\n",
    "    sim = torch.matmul(q_emb, t_emb.t())  # (N, N)\n",
    "    num_queries = sim.size(0)\n",
    "\n",
    "    recall_at_k = {k: 0 for k in k_values}\n",
    "    precision_at_k = {k: 0 for k in k_values}\n",
    "    mrr = 0.0\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        ranking = torch.argsort(sim[i], descending=True)\n",
    "        correct_idx = i\n",
    "        position = (ranking == correct_idx).nonzero(as_tuple=True)[0].item()\n",
    "        rank = position + 1\n",
    "        mrr += 1.0 / rank\n",
    "\n",
    "        for k in k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "                precision_at_k[k] += 1.0 / k\n",
    "\n",
    "    mrr /= num_queries\n",
    "    for k in k_values:\n",
    "        recall_at_k[k] /= num_queries\n",
    "        precision_at_k[k] /= num_queries\n",
    "\n",
    "    return mrr, recall_at_k, precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f409b99-fa8a-4b1e-9542-44fac96476e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "START FUSION TRAINING\n",
      "======================================================================\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4094/4094 [00:17<00:00, 239.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 1339.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6713\n",
      "üèÜ New best fusion model saved\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4094/4094 [00:12<00:00, 319.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 1351.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5664\n",
      "üèÜ New best fusion model saved\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4094/4094 [00:12<00:00, 322.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 1358.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5436\n",
      "üèÜ New best fusion model saved\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4094/4094 [00:12<00:00, 322.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 1362.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5330\n",
      "üèÜ New best fusion model saved\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4094/4094 [00:12<00:00, 322.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 1346.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5359\n",
      "‚úì Saved loss curves to fusion_losses.png\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# ============================================================\n",
    "# 8. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –≥—Ä–∞—Ñ–∏–∫–æ–º\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"START FUSION TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "\n",
    "step_losses = []        # –ª–æ—Å—Å –Ω–∞ –∫–∞–∂–¥–æ–º optimizer step\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "    fusion_encoder.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device == \"cuda\"),\n",
    "                                     dtype=torch.bfloat16):\n",
    "            q = fusion_encoder(batch['desc_audio_emb'].to(device),\n",
    "                               batch['desc_text_emb'].to(device))\n",
    "            t = fusion_encoder(batch['audio_emb'].to(device),\n",
    "                               batch['lyrics_emb'].to(device))\n",
    "            loss = clip_loss(q, t, cfg.temperature)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        step_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "        # # –∫–∞–∂–¥—ã–µ 50 —à–∞–≥–æ–≤ ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫ train loss\n",
    "        # if global_step % 50 == 0:\n",
    "        #     clear_output(wait=True)\n",
    "        #     plt.figure(figsize=(10, 4))\n",
    "        #     plt.plot(step_losses, label=\"Train loss per step\")\n",
    "        #     plt.xlabel(\"Optimizer step\")\n",
    "        #     plt.ylabel(\"Loss\")\n",
    "        #     plt.title(f\"Train loss (epoch {epoch+1}, step {global_step})\")\n",
    "        #     plt.grid(alpha=0.3)\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Val\n",
    "    val_loss = evaluate_val()\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(fusion_encoder.state_dict(), best_path)\n",
    "        print(\"üèÜ New best fusion model saved\")\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–∞–º (PNG)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(val_losses, label=\"Val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.title(\"Fusion Train/Val Loss\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(lrs, label=\"LR\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"LR\")\n",
    "plt.title(\"Learning rate\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.out_dir, \"fusion_losses.png\"))\n",
    "print(\"‚úì Saved loss curves to fusion_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a10b50f7-6bd0-407a-a36b-cdcbaf7853a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "‚úì Loaded best fusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 1530.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: torch.Size([4093, 256]) torch.Size([4093, 256])\n",
      "\n",
      "======================================================================\n",
      "FUSION RETRIEVAL METRICS (Test)\n",
      "======================================================================\n",
      "MRR: 0.1651\n",
      "Recall@1:    0.0760\n",
      "Precision@1: 0.0760\n",
      "Recall@5:    0.2516\n",
      "Precision@5: 0.0503\n",
      "Recall@10:    0.3609\n",
      "Precision@10: 0.0361\n",
      "Recall@20:    0.4698\n",
      "Precision@20: 0.0235\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. –û—Ü–µ–Ω–∫–∞ –Ω–∞ test: MRR, Recall@K, Precision@K\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fusion_encoder.load_state_dict(torch.load(best_path, map_location=device))\n",
    "fusion_encoder.to(device).eval()\n",
    "print(\"‚úì Loaded best fusion model\")\n",
    "\n",
    "q_emb, t_emb = compute_embeddings(test_loader, fusion_encoder)\n",
    "print(\"Embeddings:\", q_emb.shape, t_emb.shape)\n",
    "\n",
    "mrr, recall_at_k, precision_at_k = compute_retrieval_metrics(q_emb, t_emb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FUSION RETRIEVAL METRICS (Test)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MRR: {mrr:.4f}\")\n",
    "for k in [1, 5, 10, 20]:\n",
    "    print(f\"Recall@{k}:    {recall_at_k[k]:.4f}\")\n",
    "    print(f\"Precision@{k}: {precision_at_k[k]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4bfe3f43-385b-4571-9fda-ca03191bbdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2835, 256]) 2835\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "\n",
    "ROOT = \"/home/jovyan/persistent_volume/final_dataset\"\n",
    "\n",
    "# –≥—Ä—É–∑–∏–º —Ñ–∏—á–∏ —Ç—Ä–µ–∫–æ–≤ –∏–∑ —Ç—Ä—ë—Ö —Å–ø–ª–∏—Ç–æ–≤\n",
    "train_feats = torch.load(os.path.join(ROOT, \"train_features.pt\"))\n",
    "val_feats   = torch.load(os.path.join(ROOT, \"val_features.pt\"))\n",
    "test_feats  = torch.load(os.path.join(ROOT, \"test_features.pt\"))\n",
    "\n",
    "# –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å track_id -> —Ñ–∏—á–∏\n",
    "all_feats = {}\n",
    "all_feats.update(train_feats)\n",
    "all_feats.update(val_feats)\n",
    "all_feats.update(test_feats)\n",
    "\n",
    "all_track_ids = sorted(all_feats.keys())  # —Ñ–∏–∫—Å–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫\n",
    "\n",
    "# —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç—Ä–µ–∫–æ–≤\n",
    "audio_embs  = torch.stack([all_feats[tid][\"audio_emb\"]  for tid in all_track_ids])\n",
    "lyrics_embs = torch.stack([all_feats[tid][\"lyrics_emb\"] for tid in all_track_ids])\n",
    "\n",
    "# –ø—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ fusion_encoder, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å fused —Ç—Ä–µ–∫–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "fusion_encoder.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    t_emb_all = fusion_encoder(\n",
    "        audio_embs.to(device),\n",
    "        lyrics_embs.to(device),\n",
    "    ).cpu()   # (N_tracks, D)\n",
    "\n",
    "print(t_emb_all.shape, len(all_track_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d064799-d02b-40d1-a24e-0dbd3c6b8e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fuse test queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [00:00<00:00, 2579.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4093, 256]) 4093\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fusion_encoder.eval()\n",
    "all_q_test = []\n",
    "test_track_ids = []   # ¬´–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π¬ª —Ç—Ä–µ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Fuse test queries\"):\n",
    "        q = fusion_encoder(\n",
    "            batch[\"desc_audio_emb\"].to(device),\n",
    "            batch[\"desc_text_emb\"].to(device),\n",
    "        )\n",
    "        all_q_test.append(q.cpu())\n",
    "\n",
    "        # –±–µ—Ä–µ–º track_id –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ json, –µ—Å–ª–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –æ–Ω –µ—Å—Ç—å\n",
    "        test_track_ids.extend(batch[\"track_id\"])  # –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ __getitem__\n",
    "\n",
    "q_emb_test = torch.cat(all_q_test, dim=0)  # (N_queries, D)\n",
    "print(q_emb_test.shape, len(test_track_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e02ac547-5dae-4651-a7df-642b918cb8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.2823589794327138\n",
      "Recall@1: 0.2030295626679697\n",
      "Recall@5: 0.36501343757634985\n",
      "Recall@10: 0.4490593696555094\n",
      "Recall@20: 0.535304177864647\n",
      "Precision@1: 0.2030295626679697\n",
      "Precision@5: 0.07300268751526792\n",
      "Precision@10: 0.04490593696554944\n",
      "Precision@20: 0.026765208893231352\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "t_all_np = t_emb_all.numpy().astype(\"float32\")\n",
    "q_test_np = q_emb_test.numpy().astype(\"float32\")\n",
    "\n",
    "D = t_all_np.shape[1]\n",
    "index = faiss.IndexFlatL2(D)\n",
    "index.add(t_all_np)\n",
    "\n",
    "K = 50\n",
    "distances, indices = index.search(q_test_np, K)\n",
    "\n",
    "track_id_array = np.array(all_track_ids)\n",
    "\n",
    "def compute_faiss_metrics(indices, track_id_array, test_track_ids, ks=(1, 5, 10)):\n",
    "    N, K = indices.shape\n",
    "    ks = sorted(ks)\n",
    "    max_k = ks[-1]\n",
    "\n",
    "    mrr = 0.0\n",
    "    recall_at = {k: 0 for k in ks}\n",
    "    precision_at = {k: 0.0 for k in ks}\n",
    "\n",
    "    for i in range(N):\n",
    "        true_tid = test_track_ids[i]\n",
    "        retrieved_tids = track_id_array[indices[i, :max_k]]\n",
    "\n",
    "        hits = np.where(retrieved_tids == true_tid)[0]\n",
    "        if len(hits) > 0:\n",
    "            rank = hits[0] + 1\n",
    "            mrr += 1.0 / rank\n",
    "            for k in ks:\n",
    "                if rank <= k:\n",
    "                    recall_at[k] += 1\n",
    "\n",
    "        for k in ks:\n",
    "            topk = retrieved_tids[:k]\n",
    "            correct = np.sum(topk == true_tid)\n",
    "            precision_at[k] += correct / k\n",
    "\n",
    "    mrr /= N\n",
    "    for k in ks:\n",
    "        recall_at[k] /= N\n",
    "        precision_at[k] /= N\n",
    "\n",
    "    return mrr, recall_at, precision_at\n",
    "\n",
    "mrr, recall_at_k, precision_at_k = compute_faiss_metrics(\n",
    "    indices, track_id_array, test_track_ids, ks=(1, 5, 10, 20)\n",
    ")\n",
    "\n",
    "print(\"MRR:\", mrr)\n",
    "for k, v in recall_at_k.items():\n",
    "    print(f\"Recall@{k}:\", v)\n",
    "for k, v in precision_at_k.items():\n",
    "    print(f\"Precision@{k}:\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca801a23-d01c-49f3-a1f9-466d0134305b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
