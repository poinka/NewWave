{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e478a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import LEDTokenizer, LEDForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021d51b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d905b142c2cf4e2fa12cc031f330ae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19269528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Preprocess Datasets\n",
    "interpretation_ds = load_dataset(\"jamimulgrave/Song-Interpretation-Dataset\")['train']\n",
    "enrich_ds = load_dataset(\"seungheondoh/enrich-music4all\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c36fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "pseudo_map = {row['track_id']: row['pseudo_caption'] for row in enrich_ds}\n",
    "artist_map = {row['track_id']: row['artist_name'] for row in enrich_ds}\n",
    "tag_map = {row['track_id']: row.get('tag_list', []) for row in enrich_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08564929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from interpretation_ds\n",
    "music4all_ids = interpretation_ds['music4all_id']\n",
    "descriptions = interpretation_ds['comment']\n",
    "lyrics_list = interpretation_ds['lyrics']\n",
    "num_samples = len(music4all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e48e30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8846479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = int(0.8 * num_samples)\n",
    "val_idx = int(0.9 * num_samples)\n",
    "train_ids, train_descs, train_lyrics = music4all_ids[:train_idx], descriptions[:train_idx], lyrics_list[:train_idx]\n",
    "val_ids, val_descs, val_lyrics = music4all_ids[train_idx:val_idx], descriptions[train_idx:val_idx], lyrics_list[train_idx:val_idx]\n",
    "test_ids, test_descs, test_lyrics = music4all_ids[val_idx:], descriptions[val_idx:], lyrics_list[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43656fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(ids, real_descs, lyrics, all_lyrics, artist_map, tag_map, pseudo_map, num_neg=4, max_neg_pool=500, max_attempts=10):\n",
    "    # Check if cached pairs exist\n",
    "    cache_file = f\"pairs_{ids[0][:4]}_to_{ids[-1][:4]}.pkl\"\n",
    "    if os.path.exists(os.path.join(\"persistent_volume\", cache_file)):\n",
    "        with open(os.path.join(\"persistent_volume\", cache_file), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    # Use a subset of all_lyrics for efficiency\n",
    "    all_lyrics_subset = all_lyrics[:min(max_neg_pool, len(all_lyrics))]\n",
    "    id_lyric_map = dict(zip(ids[:max_neg_pool], all_lyrics_subset))\n",
    "    \n",
    "    # Add progress bar for the main loop\n",
    "    for i, (sid, real_desc, pos_lyric) in enumerate(tqdm(zip(ids, real_descs, lyrics), total=len(ids), desc=\"Generating pairs\")):\n",
    "        # Positive pairs: Real description + pseudo if available\n",
    "        positives.append((real_desc, pos_lyric, 1))\n",
    "        if sid in pseudo_map:\n",
    "            positives.append((pseudo_map[sid], pos_lyric, 1))\n",
    "        \n",
    "        # Generate negatives with attempt limit\n",
    "        neg_count = 0\n",
    "        artist = artist_map.get(sid, '')\n",
    "        pos_tags = tag_map.get(sid, [])\n",
    "        attempts = 0\n",
    "        \n",
    "        while neg_count < num_neg and attempts < max_attempts:\n",
    "            # Same artist negative\n",
    "            if artist and neg_count < 1:\n",
    "                same_artist_ids = [id_ for id_ in ids[:max_neg_pool] if id_ != sid and artist_map.get(id_) == artist]\n",
    "                if same_artist_ids:\n",
    "                    neg_sid = random.choice(same_artist_ids)\n",
    "                    neg_lyric = id_lyric_map[neg_sid]\n",
    "                    if neg_lyric != pos_lyric and not any(neg[1] == neg_lyric for neg in negatives):\n",
    "                        negatives.append((real_desc, neg_lyric, 0))\n",
    "                        neg_count += 1\n",
    "                        attempts = 0  # Reset attempts on success\n",
    "                    else:\n",
    "                        attempts += 1\n",
    "                else:\n",
    "                    attempts += 1\n",
    "            \n",
    "            # Tag-based negative\n",
    "            if pos_tags and neg_count < num_neg:\n",
    "                candidate_ids = [id_ for id_ in ids[:max_neg_pool] if id_ != sid and tag_map.get(id_, [])]\n",
    "                if candidate_ids:\n",
    "                    neg_sid = random.choice(candidate_ids)\n",
    "                    neg_lyric = id_lyric_map[neg_sid]\n",
    "                    if neg_lyric != pos_lyric and not any(neg[1] == neg_lyric for neg in negatives):\n",
    "                        negatives.append((real_desc, neg_lyric, 0))\n",
    "                        neg_count += 1\n",
    "                        attempts = 0  # Reset attempts on success\n",
    "                    else:\n",
    "                        attempts += 1\n",
    "                else:\n",
    "                    attempts += 1\n",
    "            \n",
    "            # Fall back to random negative\n",
    "            if neg_count < num_neg:\n",
    "                neg_lyric = random.choice(all_lyrics_subset)\n",
    "                while neg_lyric == pos_lyric:\n",
    "                    neg_lyric = random.choice(all_lyrics_subset)\n",
    "                negatives.append((real_desc, neg_lyric, 0))\n",
    "                neg_count += 1\n",
    "                attempts = 0  # Reset attempts on success\n",
    "        \n",
    "        if neg_count < num_neg:\n",
    "            print(f\"Warning: Only {neg_count} negatives found for sid {sid}, expected {num_neg}\")\n",
    "    \n",
    "    pairs = positives + negatives\n",
    "    random.shuffle(pairs)\n",
    "    \n",
    "    # Save to cache\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(pairs, f)\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdda6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = lyrics_list\n",
    "train_pairs = generate_pairs(train_ids, train_descs, train_lyrics, all_lyrics, artist_map, tag_map, pseudo_map)\n",
    "val_pairs = generate_pairs(val_ids, val_descs, val_lyrics, all_lyrics, artist_map, tag_map, pseudo_map)\n",
    "test_pairs = generate_pairs(test_ids, test_descs, test_lyrics, all_lyrics, artist_map, tag_map, pseudo_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f276843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class LyricsMatcherDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=1024):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        desc, lyric, label = self.pairs[idx]\n",
    "        input_text = f\"[CLS] {desc} [SEP] {lyric}\"\n",
    "        encoding = self.tokenizer(input_text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e22790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/led/modeling_led.py:2496: FutureWarning: The `transformers.LEDForSequenceClassification` class is deprecated and will be removed in version 5 of Transformers. No actual method were provided in the original paper on how to perfom sequence classification.\n",
      "  warnings.warn(\n",
      "Some weights of LEDForSequenceClassification were not initialized from the model checkpoint at allenai/led-base-16384 and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LEDForSequenceClassification(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): LEDClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Setup Model and Tokenizer\n",
    "tokenizer = LEDTokenizer.from_pretrained('allenai/led-base-16384')\n",
    "model = LEDForSequenceClassification.from_pretrained('allenai/led-base-16384', num_labels=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "115f1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_dataset = LyricsMatcherDataset(train_pairs, tokenizer)\n",
    "val_dataset = LyricsMatcherDataset(val_pairs, tokenizer)\n",
    "test_dataset = LyricsMatcherDataset(test_pairs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size for memory\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5783e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Training\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = 5\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b553dd5-2ba7-4175-9aa6-f6091a5b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last checkpoint if exists\n",
    "checkpoint_path = \"persistent_volume/last_checkpoint.pth\"\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8563806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/357183 [00:59<143:36:56,  1.45s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).unsqueeze(1)\n",
    "        \n",
    "        try:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_losses[-1]:.4f}, Val Loss {val_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ba3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Testing and Metrics\n",
    "def compute_ranking_metrics(model, descs, lyrics, all_lyrics):\n",
    "    model.eval()\n",
    "    mrr = 0\n",
    "    recall_at_5 = 0\n",
    "    recall_at_10 = 0\n",
    "    num_queries = len(descs)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, desc in enumerate(tqdm(descs)):\n",
    "            scores = []\n",
    "            for lyric in all_lyrics:\n",
    "                input_text = f\"[CLS] {desc} [SEP] {lyric}\"\n",
    "                encoding = tokenizer(input_text, truncation=True, max_length=16384, padding='max_length', return_tensors='pt').to(device)\n",
    "                output = model(**encoding)\n",
    "                score = torch.sigmoid(output.logits).item()\n",
    "                scores.append(score)\n",
    "            \n",
    "            ranked_indices = np.argsort(scores)[::-1]\n",
    "            pos_rank = np.where(ranked_indices == all_lyrics.index(lyrics[i]))[0][0] + 1\n",
    "            \n",
    "            mrr += 1 / pos_rank\n",
    "            recall_at_5 += 1 if pos_rank <= 5 else 0\n",
    "            recall_at_10 += 1 if pos_rank <= 10 else 0\n",
    "    \n",
    "    return {\n",
    "        'MRR': mrr / num_queries,\n",
    "        'Recall@5': recall_at_5 / num_queries,\n",
    "        'Recall@10': recall_at_10 / num_queries\n",
    "    }\n",
    "\n",
    "test_metrics = compute_ranking_metrics(model, test_descs, test_lyrics, all_lyrics[:100])  # Subset for demo\n",
    "print(\"Test Metrics:\", test_metrics)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).squeeze() > 0.5\n",
    "        correct += (preds == labels.bool()).sum().item()\n",
    "        total += len(labels)\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# Step 6: Plots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_curve.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
