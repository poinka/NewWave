{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bi-Encoder Architecture for Lyrics-Description Matching\n",
        "## Улучшенная архитектура с раздельными энкодерами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "from typing import Dict, List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "interpretation_ds = load_dataset(\"jamimulgrave/Song-Interpretation-Dataset\")['train']\n",
        "enrich_ds = load_dataset(\"seungheondoh/enrich-music4all\")['train']\n",
        "\n",
        "print(f\"Interpretation dataset size: {len(interpretation_ds)}\")\n",
        "print(f\"Enrich dataset size: {len(enrich_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create mappings\n",
        "pseudo_map = {row['track_id']: row['pseudo_caption'] for row in enrich_ds}\n",
        "artist_map = {row['track_id']: row['artist_name'] for row in enrich_ds}\n",
        "tag_map = {row['track_id']: row.get('tag_list', []) for row in enrich_ds}\n",
        "\n",
        "print(f\"Mappings created: {len(pseudo_map)} tracks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract data\n",
        "music4all_ids = interpretation_ds['music4all_id']\n",
        "descriptions = interpretation_ds['comment']\n",
        "lyrics_list = interpretation_ds['lyrics']\n",
        "num_samples = len(music4all_ids)\n",
        "\n",
        "print(f\"Total samples: {num_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Val/Test split\n",
        "train_idx = int(0.8 * num_samples)\n",
        "val_idx = int(0.9 * num_samples)\n",
        "\n",
        "train_ids = music4all_ids[:train_idx]\n",
        "train_descs = descriptions[:train_idx]\n",
        "train_lyrics = lyrics_list[:train_idx]\n",
        "\n",
        "val_ids = music4all_ids[train_idx:val_idx]\n",
        "val_descs = descriptions[train_idx:val_idx]\n",
        "val_lyrics = lyrics_list[train_idx:val_idx]\n",
        "\n",
        "test_ids = music4all_ids[val_idx:]\n",
        "test_descs = descriptions[val_idx:]\n",
        "test_lyrics = lyrics_list[val_idx:]\n",
        "\n",
        "print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bi-Encoder Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiEncoder(nn.Module):\n",
        "    \"\"\"Bi-Encoder with separate encoders for descriptions and lyrics.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name='allenai/longformer-base-4096', embedding_dim=768, projection_dim=512):\n",
        "        super(BiEncoder, self).__init__()\n",
        "        \n",
        "        # Shared base encoder (can be split into two if needed)\n",
        "        self.desc_encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.lyrics_encoder = AutoModel.from_pretrained(model_name)\n",
        "        \n",
        "        # Projection heads for contrastive learning\n",
        "        self.desc_projection = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "        \n",
        "        self.lyrics_projection = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "        \n",
        "    def encode_description(self, input_ids, attention_mask):\n",
        "        \"\"\"Encode description text.\"\"\"\n",
        "        outputs = self.desc_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Use [CLS] token embedding\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]\n",
        "        projected = self.desc_projection(pooled)\n",
        "        # L2 normalize for cosine similarity\n",
        "        return F.normalize(projected, p=2, dim=1)\n",
        "    \n",
        "    def encode_lyrics(self, input_ids, attention_mask):\n",
        "        \"\"\"Encode lyrics text.\"\"\"\n",
        "        outputs = self.lyrics_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Use [CLS] token embedding\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]\n",
        "        projected = self.lyrics_projection(pooled)\n",
        "        # L2 normalize for cosine similarity\n",
        "        return F.normalize(projected, p=2, dim=1)\n",
        "    \n",
        "    def forward(self, desc_input_ids, desc_attention_mask, lyrics_input_ids, lyrics_attention_mask):\n",
        "        \"\"\"Forward pass returns both embeddings.\"\"\"\n",
        "        desc_emb = self.encode_description(desc_input_ids, desc_attention_mask)\n",
        "        lyrics_emb = self.encode_lyrics(lyrics_input_ids, lyrics_attention_mask)\n",
        "        return desc_emb, lyrics_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contrastive Loss with In-Batch Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"InfoNCE / NT-Xent loss for contrastive learning.\"\"\"\n",
        "    \n",
        "    def __init__(self, temperature=0.07):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def forward(self, desc_embeddings, lyrics_embeddings):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            desc_embeddings: [batch_size, embedding_dim]\n",
        "            lyrics_embeddings: [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        batch_size = desc_embeddings.shape[0]\n",
        "        \n",
        "        # Compute similarity matrix\n",
        "        # [batch_size, batch_size]\n",
        "        similarity_matrix = torch.matmul(desc_embeddings, lyrics_embeddings.T) / self.temperature\n",
        "        \n",
        "        # Labels: diagonal elements are positive pairs\n",
        "        labels = torch.arange(batch_size, device=desc_embeddings.device)\n",
        "        \n",
        "        # Bidirectional loss (description->lyrics and lyrics->description)\n",
        "        loss_desc = self.criterion(similarity_matrix, labels)\n",
        "        loss_lyrics = self.criterion(similarity_matrix.T, labels)\n",
        "        \n",
        "        return (loss_desc + loss_lyrics) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset for Bi-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiEncoderDataset(Dataset):\n",
        "    \"\"\"Dataset that tokenizes descriptions and lyrics separately.\"\"\"\n",
        "    \n",
        "    def __init__(self, descriptions: List[str], lyrics: List[str], tokenizer, \n",
        "                 max_desc_length=512, max_lyrics_length=4096):\n",
        "        self.descriptions = descriptions\n",
        "        self.lyrics = lyrics\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_desc_length = max_desc_length\n",
        "        self.max_lyrics_length = max_lyrics_length\n",
        "        \n",
        "        assert len(descriptions) == len(lyrics), \"Descriptions and lyrics must have same length\"\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.descriptions)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        desc = self.descriptions[idx]\n",
        "        lyric = self.lyrics[idx]\n",
        "        \n",
        "        # Tokenize description\n",
        "        desc_encoding = self.tokenizer(\n",
        "            desc,\n",
        "            truncation=True,\n",
        "            max_length=self.max_desc_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Tokenize lyrics\n",
        "        lyrics_encoding = self.tokenizer(\n",
        "            lyric,\n",
        "            truncation=True,\n",
        "            max_length=self.max_lyrics_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'desc_input_ids': desc_encoding['input_ids'].squeeze(0),\n",
        "            'desc_attention_mask': desc_encoding['attention_mask'].squeeze(0),\n",
        "            'lyrics_input_ids': lyrics_encoding['input_ids'].squeeze(0),\n",
        "            'lyrics_attention_mask': lyrics_encoding['attention_mask'].squeeze(0)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "model = BiEncoder(model_name='allenai/longformer-base-4096', embedding_dim=768, projection_dim=512)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = BiEncoderDataset(train_descs, train_lyrics, tokenizer, \n",
        "                                  max_desc_length=512, max_lyrics_length=4096)\n",
        "val_dataset = BiEncoderDataset(val_descs, val_lyrics, tokenizer, \n",
        "                                max_desc_length=512, max_lyrics_length=4096)\n",
        "test_dataset = BiEncoderDataset(test_descs, test_lyrics, tokenizer, \n",
        "                                 max_desc_length=512, max_lyrics_length=4096)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)}\")\n",
        "print(f\"Val dataset: {len(val_dataset)}\")\n",
        "print(f\"Test dataset: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "batch_size = 8  # Adjust based on GPU memory\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 500\n",
        "temperature = 0.07\n",
        "\n",
        "# Initialize optimizer and loss\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "criterion = ContrastiveLoss(temperature=temperature)\n",
        "\n",
        "# Learning rate scheduler with warmup\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=learning_rate,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.1,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "print(\"Training setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training state\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "checkpoint_dir = 'persistent_volume'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Load checkpoint if exists\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'bi_encoder_checkpoint.pth')\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    train_losses = checkpoint.get('train_losses', [])\n",
        "    val_losses = checkpoint.get('val_losses', [])\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    print(f\"Resuming from epoch {start_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move batch to device\n",
        "        desc_input_ids = batch['desc_input_ids'].to(device)\n",
        "        desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
        "        lyrics_input_ids = batch['lyrics_input_ids'].to(device)\n",
        "        lyrics_attention_mask = batch['lyrics_attention_mask'].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        desc_emb, lyrics_emb = model(desc_input_ids, desc_attention_mask, \n",
        "                                      lyrics_input_ids, lyrics_attention_mask)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(desc_emb, lyrics_emb)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "    \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate on validation/test set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            desc_input_ids = batch['desc_input_ids'].to(device)\n",
        "            desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
        "            lyrics_input_ids = batch['lyrics_input_ids'].to(device)\n",
        "            lyrics_attention_mask = batch['lyrics_attention_mask'].to(device)\n",
        "            \n",
        "            desc_emb, lyrics_emb = model(desc_input_ids, desc_attention_mask, \n",
        "                                          lyrics_input_ids, lyrics_attention_mask)\n",
        "            \n",
        "            loss = criterion(desc_emb, lyrics_emb)\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_val_loss': best_val_loss\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_path = os.path.join(checkpoint_dir, 'bi_encoder_best.pth')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"✓ New best model saved! Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embeddings(model, dataloader, device):\n",
        "    \"\"\"Compute embeddings for all samples.\"\"\"\n",
        "    model.eval()\n",
        "    desc_embeddings = []\n",
        "    lyrics_embeddings = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Computing embeddings\"):\n",
        "            desc_input_ids = batch['desc_input_ids'].to(device)\n",
        "            desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
        "            lyrics_input_ids = batch['lyrics_input_ids'].to(device)\n",
        "            lyrics_attention_mask = batch['lyrics_attention_mask'].to(device)\n",
        "            \n",
        "            desc_emb, lyrics_emb = model(desc_input_ids, desc_attention_mask, \n",
        "                                          lyrics_input_ids, lyrics_attention_mask)\n",
        "            \n",
        "            desc_embeddings.append(desc_emb.cpu())\n",
        "            lyrics_embeddings.append(lyrics_emb.cpu())\n",
        "    \n",
        "    desc_embeddings = torch.cat(desc_embeddings, dim=0)\n",
        "    lyrics_embeddings = torch.cat(lyrics_embeddings, dim=0)\n",
        "    \n",
        "    return desc_embeddings, lyrics_embeddings\n",
        "\n",
        "\n",
        "def compute_retrieval_metrics(desc_embeddings, lyrics_embeddings, k_values=[1, 5, 10]):\n",
        "    \"\"\"Compute retrieval metrics (Recall@K, MRR).\"\"\"\n",
        "    # Compute similarity matrix\n",
        "    similarity_matrix = torch.matmul(desc_embeddings, lyrics_embeddings.T)\n",
        "    \n",
        "    # For each description, rank lyrics by similarity\n",
        "    num_queries = similarity_matrix.shape[0]\n",
        "    \n",
        "    recall_at_k = {k: 0 for k in k_values}\n",
        "    mrr = 0\n",
        "    \n",
        "    for i in range(num_queries):\n",
        "        # Get ranking (indices sorted by similarity)\n",
        "        ranking = torch.argsort(similarity_matrix[i], descending=True)\n",
        "        \n",
        "        # Find position of correct match (diagonal element)\n",
        "        correct_idx = i\n",
        "        position = (ranking == correct_idx).nonzero(as_tuple=True)[0].item()\n",
        "        rank = position + 1\n",
        "        \n",
        "        # Update MRR\n",
        "        mrr += 1.0 / rank\n",
        "        \n",
        "        # Update Recall@K\n",
        "        for k in k_values:\n",
        "            if rank <= k:\n",
        "                recall_at_k[k] += 1\n",
        "    \n",
        "    # Normalize\n",
        "    mrr /= num_queries\n",
        "    for k in k_values:\n",
        "        recall_at_k[k] /= num_queries\n",
        "    \n",
        "    return mrr, recall_at_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for evaluation\n",
        "best_model_path = os.path.join(checkpoint_dir, 'bi_encoder_best.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(\"Best model loaded for evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute test embeddings\n",
        "test_desc_emb, test_lyrics_emb = compute_embeddings(model, test_loader, device)\n",
        "\n",
        "print(f\"Description embeddings shape: {test_desc_emb.shape}\")\n",
        "print(f\"Lyrics embeddings shape: {test_lyrics_emb.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute retrieval metrics\n",
        "mrr, recall_at_k = compute_retrieval_metrics(test_desc_emb, test_lyrics_emb, k_values=[1, 5, 10, 20])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RETRIEVAL METRICS (Test Set)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"MRR: {mrr:.4f}\")\n",
        "for k, value in recall_at_k.items():\n",
        "    print(f\"Recall@{k}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(val_losses, label='Val Loss', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "k_list = list(recall_at_k.keys())\n",
        "recall_values = [recall_at_k[k] for k in k_list]\n",
        "plt.bar(k_list, recall_values, color='skyblue')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Recall@K')\n",
        "plt.title('Retrieval Performance')\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(checkpoint_dir, 'training_results.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_model_path = os.path.join(checkpoint_dir, 'bi_encoder_final.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'test_mrr': mrr,\n",
        "    'test_recall': recall_at_k,\n",
        "    'hyperparameters': {\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size,\n",
        "        'temperature': temperature,\n",
        "        'num_epochs': num_epochs\n",
        "    }\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"Final model saved to {final_model_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}