{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26068ef8-a94e-45fe-a941-a3a726ed6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor, ClapModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ade16a-e612-4fb0-8956-95903f7bb49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. –ö–æ–Ω—Ñ–∏–≥\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_json: str = \"final_dataset/train.json\"\n",
    "    val_json: str   = \"final_dataset/val.json\"\n",
    "    test_json: str  = \"final_dataset/test.json\"\n",
    "\n",
    "    text_model_name: str = \"BAAI/bge-m3\"\n",
    "    projection_dim: int = 2048\n",
    "    dropout: float = 0.3\n",
    "    biencoder_ckpt: str = \"25_ep_tag_hard_negs_bge_8192/bi_encoder_best.pth\"\n",
    "\n",
    "    clap_ckpt_dir: str = \"clap\"  # dir —Å save_pretrained() –∏–∑ Jamendo‚Äë—Å–∫—Ä–∏–ø—Ç–∞\n",
    "\n",
    "    audio_sr: int = 48000\n",
    "    max_audio_seconds: int = 30\n",
    "\n",
    "    max_desc_len: int = 4096\n",
    "    max_lyrics_len: int = 4096\n",
    "\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 0\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    fused_dim: int = 512\n",
    "    temperature: float = 0.07\n",
    "\n",
    "    out_dir: str = \"fusion_ckpts\"\n",
    "    val_log_path: str = \"fusion_val_losses.json\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "AUDIO_ROOT = os.path.expanduser('~/persistent_volume/final_dataset/audio/audio')  # –∏–ª–∏ –ø—É—Ç—å –≤ –∫–æ–ª–∞–±–µ/–∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5b550-87a3-4a82-a2bf-3447f2a9295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in /home/jovyan/persistent_volume/final_dataset/train.json: 32760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features:   4%|‚ñé         | 1184/32760 [39:34<17:30:11,  2.00s/it]"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor, ClapModel\n",
    "import torchaudio\n",
    "\n",
    "# –ø—É—Ç–∏\n",
    "PROJECT_DIR = os.path.expanduser('~/persistent_volume')\n",
    "AUDIO_ROOT = os.path.join(PROJECT_DIR, 'final_dataset', 'audio/audio')\n",
    "\n",
    "DATA_JSON = os.path.join(PROJECT_DIR, 'final_dataset', 'train.json')\n",
    "OUT_FEATS = os.path.join(PROJECT_DIR, 'final_dataset', 'train_features.pt')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- –º–æ–¥–µ–ª–∏ ---\n",
    "\n",
    "text_tok = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "text_backbone = AutoModel.from_pretrained(cfg.text_model_name, trust_remote_code=True).to(device).eval()\n",
    "text_backbone.gradient_checkpointing_enable()\n",
    "biencoder = BiEncoder(text_backbone, cfg.projection_dim, cfg.dropout).to(device)\n",
    "\n",
    "biencoder.load_state_dict(torch.load(cfg.biencoder_ckpt, map_location=device))\n",
    "biencoder.to(device).eval()\n",
    "\n",
    "clap_processor = AutoProcessor.from_pretrained(cfg.clap_ckpt_dir)\n",
    "clap_model = ClapModel.from_pretrained(cfg.clap_ckpt_dir).to(device).eval()\n",
    "\n",
    "clap_sr = clap_processor.feature_extractor.sampling_rate\n",
    "\n",
    "# --- –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---\n",
    "\n",
    "with open(DATA_JSON, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total items in {DATA_JSON}: {len(data)}\")\n",
    "\n",
    "all_feats = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_batch(batch_items):\n",
    "    \"\"\"batch_items: —Å–ø–∏—Å–æ–∫ dict {track_id, lyrics, description}\"\"\"\n",
    "    # 1) —á–∏—Ç–∞–µ–º –∏ –≥–æ—Ç–æ–≤–∏–º –∞—É–¥–∏–æ (—Å–ø–∏—Å–æ–∫ numpy/—Ç–µ–Ω–∑–æ—Ä–æ–≤)\n",
    "    wavs = []\n",
    "    tids = []\n",
    "    for item in batch_items:\n",
    "        tid = item['track_id']\n",
    "        path = os.path.join(AUDIO_ROOT, f\"{tid}.mp3\")\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if sr != clap_sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, clap_sr)\n",
    "        if wav.size(0) > 1:\n",
    "            wav = wav.mean(0, keepdim=True)\n",
    "        wavs.append(wav.squeeze(0).cpu().numpy())\n",
    "        tids.append(tid)\n",
    "\n",
    "    # 2) CLAP audio —Ñ–∏—á–∏ (–±–∞—Ç—á)\n",
    "    audio_inputs = clap_processor.feature_extractor(\n",
    "        raw_speech=wavs,\n",
    "        sampling_rate=clap_sr,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    audio_inputs = {k: v.to(device) for k, v in audio_inputs.items()}\n",
    "    audio_emb = clap_model.get_audio_features(\n",
    "        input_features=audio_inputs['input_features']\n",
    "    )                      # (B, D_A)\n",
    "\n",
    "    # 3) —Ç–µ–∫—Å—Ç—ã –±–∞—Ç—á–µ–º\n",
    "    lyrics_list = [it['lyrics'] for it in batch_items]\n",
    "    desc_list   = [it['description'] for it in batch_items]\n",
    "\n",
    "    lyr_enc = text_tok(\n",
    "        lyrics_list, truncation=True, padding=True,\n",
    "        max_length=cfg.max_lyrics_len, return_tensors='pt'\n",
    "    ).to(device)\n",
    "    desc_enc = text_tok(\n",
    "        desc_list, truncation=True, padding=True,\n",
    "        max_length=cfg.max_desc_len, return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    lyr_emb = biencoder.encode_lyrics(\n",
    "        lyr_enc['input_ids'], lyr_enc['attention_mask']\n",
    "    )                     # (B, D_T)\n",
    "    desc_txt_emb = biencoder.encode_description(\n",
    "        desc_enc['input_ids'], desc_enc['attention_mask']\n",
    "    )                     # (B, D_T)\n",
    "\n",
    "    # 4) CLAP text –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π –±–∞—Ç—á–µ–º\n",
    "    clap_txt = clap_processor(\n",
    "        text=desc_list, return_tensors='pt', padding=True, truncation=True\n",
    "    )\n",
    "    clap_txt = {k: v.to(device) for k, v in clap_txt.items()}\n",
    "    desc_audio_emb = clap_model.get_text_features(\n",
    "        input_ids=clap_txt['input_ids'],\n",
    "        attention_mask=clap_txt['attention_mask'],\n",
    "    )                     # (B, D_A)\n",
    "\n",
    "    # 5) —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –ø–æ track_id\n",
    "    for i, tid in enumerate(tids):\n",
    "        all_feats[tid] = {\n",
    "            'audio_emb':      audio_emb[i].cpu(),\n",
    "            'lyrics_emb':     lyr_emb[i].cpu(),\n",
    "            'desc_audio_emb': desc_audio_emb[i].cpu(),\n",
    "            'desc_text_emb':  desc_txt_emb[i].cpu(),\n",
    "        }\n",
    "\n",
    "# --- –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ –±–∞—Ç—á–∞–º ---\n",
    "\n",
    "batch = []\n",
    "for item in tqdm(data, desc=\"Precomputing features\"):\n",
    "    batch.append({\n",
    "        'track_id':   item['track_id'],\n",
    "        'lyrics':     item['lyrics'],\n",
    "        'description': item['description'],\n",
    "    })\n",
    "    if len(batch) == BATCH_SIZE:\n",
    "        process_batch(batch)\n",
    "        batch = []\n",
    "\n",
    "# —Ö–≤–æ—Å—Ç\n",
    "if batch:\n",
    "    process_batch(batch)\n",
    "\n",
    "print(f\"Computed features for {len(all_feats)} tracks\")\n",
    "\n",
    "torch.save(all_feats, OUT_FEATS)\n",
    "print(f\"Saved features to {OUT_FEATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9d757d-5608-4d1d-b54c-dd7d5561b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. BiEncoder\n",
    "# ============================================================\n",
    "\n",
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, backbone, projection_dim, p_drop):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        emb_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        def head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(emb_dim, projection_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p_drop),\n",
    "                nn.Linear(projection_dim, projection_dim),\n",
    "                nn.Dropout(p_drop),\n",
    "            )\n",
    "\n",
    "        self.desc_head = head()\n",
    "        self.lyr_head = head()\n",
    "\n",
    "    def _mean_pool(self, outputs, attention_mask):\n",
    "        # –∫–∞–∫ —É —Ç–µ–±—è: –±–µ–∑ autocast, –≤ float32 [file:22]\n",
    "        hs = outputs.last_hidden_state.float()\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        denom = mask.sum(1).clamp_min(1e-6)\n",
    "        pooled = (hs * mask).sum(1) / denom\n",
    "        return pooled\n",
    "\n",
    "    def encode_description(self, ids, mask):\n",
    "        out = self.backbone(input_ids=ids, attention_mask=mask)\n",
    "        proj = self.desc_head(self._mean_pool(out, mask))\n",
    "        return F.normalize(proj, p=2, dim=1)\n",
    "\n",
    "    def encode_lyrics(self, ids, mask):\n",
    "        out = self.backbone(input_ids=ids, attention_mask=mask)\n",
    "        proj = self.lyr_head(self._mean_pool(out, mask))\n",
    "        return F.normalize(proj, p=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf858657-b299-40a0-a343-1ff734757cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BiEncoder backbone/tokenizer...\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "text_backbone = AutoModel.from_pretrained(cfg.text_model_name, trust_remote_code=True)\n",
    "text_backbone.gradient_checkpointing_enable()\n",
    "biencoder = BiEncoder(text_backbone, cfg.projection_dim, cfg.dropout).to(device)\n",
    "\n",
    "# –≥—Ä—É–∑–∏–º —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤—ã, backbone –∑–∞–º–æ—Ä–æ–∑–∏–º\n",
    "state = torch.load(cfg.biencoder_ckpt, map_location=\"cpu\")\n",
    "biencoder.load_state_dict(state)\n",
    "for p in biencoder.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "biencoder.eval()\n",
    "print(\"‚úì BiEncoder loaded from\", cfg.biencoder_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3e0f7-e937-46e6-a43f-3c8b547e9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. CLAP\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading CLAP model & processor...\")\n",
    "clap_processor = AutoProcessor.from_pretrained(cfg.clap_ckpt_dir)\n",
    "clap_model = ClapModel.from_pretrained(cfg.clap_ckpt_dir).to(device)\n",
    "clap_model.eval()\n",
    "for p in clap_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"‚úì CLAP loaded from\", cfg.clap_ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0f2f7-f2c4-47b8-9417-0416170c4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Fusion encoder (concat + MLP)\n",
    "# ============================================================\n",
    "\n",
    "D_AUDIO = clap_model.config.projection_dim  # –æ–±—ã—á–Ω–æ 512 [file:139]\n",
    "D_TEXT = cfg.projection_dim                # 2048 –∏–∑ BiEncoder\n",
    "\n",
    "class FusionEncoder(nn.Module):\n",
    "    def __init__(self, dim_audio, dim_text, fused_dim):\n",
    "        super().__init__()\n",
    "        in_dim = dim_audio + dim_text\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, fused_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(fused_dim, fused_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        x = torch.cat([audio_emb, text_emb], dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "fusion_encoder = FusionEncoder(D_AUDIO, D_TEXT, cfg.fused_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd3a8f-7825-4af8-94b2-8e97c11d9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. Dataset: audio + lyrics + description\n",
    "# ============================================================\n",
    "\n",
    "class FusionFeatDataset(Dataset):\n",
    "    def __init__(self, json_path, feats_path):\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.items = json.load(f)\n",
    "        self.feats = torch.load(feats_path)  # dict tid -> tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        tid = item['track_id']\n",
    "        f = self.feats[tid]\n",
    "        return {\n",
    "            'audio_emb': f['audio_emb'],\n",
    "            'lyrics_emb': f['lyrics_emb'],\n",
    "            'desc_audio_emb': f['desc_audio_emb'],\n",
    "            'desc_text_emb': f['desc_text_emb'],\n",
    "        }\n",
    "\n",
    "def collate_fn_feats(batch):\n",
    "    return {\n",
    "        'audio_emb': torch.stack([b['audio_emb'] for b in batch]),\n",
    "        'lyrics_emb': torch.stack([b['lyrics_emb'] for b in batch]),\n",
    "        'desc_audio_emb': torch.stack([b['desc_audio_emb'] for b in batch]),\n",
    "        'desc_text_emb': torch.stack([b['desc_text_emb'] for b in batch]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032a4d8-f9e5-4678-b63d-6445ea02fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FusionDataset(cfg.train_json)\n",
    "val_ds   = FusionDataset(cfg.val_json)\n",
    "test_ds  = FusionDataset(cfg.test_json)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2c147-25c8-498d-bc5b-114f13e2b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Loss –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "# ============================================================\n",
    "\n",
    "def clip_loss(q, t, temperature):\n",
    "    logits = (q @ t.t()) / temperature\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "\n",
    "optimizer = AdamW(fusion_encoder.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=cfg.lr,\n",
    "                       epochs=cfg.epochs, steps_per_epoch=len(train_loader))\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c26e5e-00ef-47e3-88e2-46a447663fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "best_path = os.path.join(cfg.out_dir, \"fusion_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94faf7-c4c4-44c0-994d-c2497e4965ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: encode –∏ metrics\n",
    "# ============================================================\n",
    "\n",
    "def encode_batch(batch):\n",
    "    audio = batch[\"audio\"].to(device)\n",
    "    lyr_ids = batch[\"lyrics_input_ids\"].to(device)\n",
    "    lyr_mask = batch[\"lyrics_attention_mask\"].to(device)\n",
    "    desc_ids = batch[\"desc_input_ids\"].to(device)\n",
    "    desc_mask = batch[\"desc_attention_mask\"].to(device)\n",
    "    clap_ids = batch[\"clap_input_ids\"].to(device)\n",
    "    clap_mask = batch[\"clap_attention_mask\"]\n",
    "    if clap_mask is not None:\n",
    "        clap_mask = clap_mask.to(device)\n",
    "\n",
    "    # –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã —Å—á–∏—Ç–∞–µ–º –ø–æ–¥ no_grad\n",
    "    with torch.no_grad():\n",
    "        audio_inputs = clap_processor.feature_extractor(\n",
    "            raw_speech=audio.squeeze(1).cpu().numpy(),\n",
    "            sampling_rate=clap_processor.feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        audio_feat = clap_model.get_audio_features(\n",
    "            input_features=audio_inputs[\"input_features\"].to(device)\n",
    "        )                           # (B, D_A)\n",
    "        audio_feat = F.normalize(audio_feat, p=2, dim=-1)\n",
    "\n",
    "        lyr_emb = biencoder.encode_lyrics(lyr_ids, lyr_mask)          # (B, D_T)\n",
    "        desc_emb = biencoder.encode_description(desc_ids, desc_mask)  # (B, D_T)\n",
    "\n",
    "        clap_text_feat = clap_model.get_text_features(\n",
    "            input_ids=clap_ids,\n",
    "            attention_mask=clap_mask,\n",
    "        )                           # (B, D_A)\n",
    "        clap_text_feat = F.normalize(clap_text_feat, p=2, dim=-1)\n",
    "\n",
    "    # fusion_encoder –±–µ–∑ no_grad\n",
    "    track_fused = fusion_encoder(audio_feat, lyr_emb)        # (B, D_fused)\n",
    "    query_fused = fusion_encoder(clap_text_feat, desc_emb)   # (B, D_fused)\n",
    "\n",
    "    return query_fused, track_fused\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_val():\n",
    "    fusion_encoder.eval()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        q, t = encode_batch(batch)\n",
    "        loss = clip_loss(q, t, cfg.temperature)\n",
    "        total_loss += loss.item()\n",
    "    avg = total_loss / len(val_loader)\n",
    "    # –ª–æ–≥ –≤ json –∫–∞–∫ –≤ —Ç–≤–æ—ë–º evaluate() [attached_file:22]\n",
    "    try:\n",
    "        with open(cfg.val_log_path, \"r\") as f:\n",
    "            logs = json.load(f)\n",
    "    except Exception:\n",
    "        logs = []\n",
    "    logs.append({\"epoch\": len(logs), \"val_loss\": avg, \"timestamp\": str(datetime.now())})\n",
    "    with open(cfg.val_log_path, \"w\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "    return avg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(loader):\n",
    "    fusion_encoder.eval()\n",
    "    all_q, all_t = [], []\n",
    "    for batch in tqdm(loader, desc=\"Computing embeddings\"):\n",
    "        q, t = encode_batch(batch)\n",
    "        all_q.append(q)\n",
    "        all_t.append(t)\n",
    "    all_q = torch.cat(all_q, dim=0)\n",
    "    all_t = torch.cat(all_t, dim=0)\n",
    "    return all_q, all_t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_retrieval_metrics(q_emb, t_emb, k_values=(1, 5, 10, 20)):\n",
    "    sim = torch.matmul(q_emb, t_emb.t())  # (N, N)\n",
    "    num_queries = sim.size(0)\n",
    "\n",
    "    recall_at_k = {k: 0 for k in k_values}\n",
    "    precision_at_k = {k: 0 for k in k_values}\n",
    "    mrr = 0.0\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        ranking = torch.argsort(sim[i], descending=True)\n",
    "        correct_idx = i\n",
    "        position = (ranking == correct_idx).nonzero(as_tuple=True)[0].item()\n",
    "        rank = position + 1\n",
    "        mrr += 1.0 / rank\n",
    "\n",
    "        for k in k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "                precision_at_k[k] += 1.0 / k\n",
    "\n",
    "    mrr /= num_queries\n",
    "    for k in k_values:\n",
    "        recall_at_k[k] /= num_queries\n",
    "        precision_at_k[k] /= num_queries\n",
    "\n",
    "    return mrr, recall_at_k, precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f409b99-fa8a-4b1e-9542-44fac96476e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# ============================================================\n",
    "# 8. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –≥—Ä–∞—Ñ–∏–∫–æ–º\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"START FUSION TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "\n",
    "step_losses = []        # –ª–æ—Å—Å –Ω–∞ –∫–∞–∂–¥–æ–º optimizer step\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "    fusion_encoder.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\"),\n",
    "                                     dtype=torch.bfloat16):\n",
    "            q = fusion_encoder(batch['desc_audio_emb'].to(device),\n",
    "                               batch['desc_text_emb'].to(device))\n",
    "            t = fusion_encoder(batch['audio_emb'].to(device),\n",
    "                               batch['lyrics_emb'].to(device))\n",
    "            loss = clip_loss(q, t, cfg.temperature)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        step_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "        # –∫–∞–∂–¥—ã–µ 50 —à–∞–≥–æ–≤ ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫ train loss\n",
    "        if global_step % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(step_losses, label=\"Train loss per step\")\n",
    "            plt.xlabel(\"Optimizer step\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(f\"Train loss (epoch {epoch+1}, step {global_step})\")\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Val\n",
    "    val_loss = evaluate_val()\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(fusion_encoder.state_dict(), best_path)\n",
    "        print(\"üèÜ New best fusion model saved\")\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–∞–º (PNG)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(val_losses, label=\"Val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.title(\"Fusion Train/Val Loss\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(lrs, label=\"LR\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"LR\")\n",
    "plt.title(\"Learning rate\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.out_dir, \"fusion_losses.png\"))\n",
    "print(\"‚úì Saved loss curves to fusion_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b50f7-6bd0-407a-a36b-cdcbaf7853a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. –û—Ü–µ–Ω–∫–∞ –Ω–∞ test: MRR, Recall@K, Precision@K\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fusion_encoder.load_state_dict(torch.load(best_path, map_location=device))\n",
    "fusion_encoder.to(device).eval()\n",
    "print(\"‚úì Loaded best fusion model\")\n",
    "\n",
    "q_emb, t_emb = compute_embeddings(test_loader)\n",
    "print(\"Embeddings:\", q_emb.shape, t_emb.shape)\n",
    "\n",
    "mrr, recall_at_k, precision_at_k = compute_retrieval_metrics(q_emb, t_emb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FUSION RETRIEVAL METRICS (Test)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MRR: {mrr:.4f}\")\n",
    "for k in [1, 5, 10, 20]:\n",
    "    print(f\"Recall@{k}:    {recall_at_k[k]:.4f}\")\n",
    "    print(f\"Precision@{k}: {precision_at_k[k]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a2fda-3428-4a85-9f22-57b62f48244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torchaudio, os\n",
    "\n",
    "bad = []\n",
    "\n",
    "for i in tqdm(range(len(train_ds))):\n",
    "    item = train_ds.data[i]\n",
    "    tid = item['track_id']\n",
    "    path = os.path.join(AUDIO_ROOT, f\"{tid}.mp3\")\n",
    "    if not os.path.exists(path):\n",
    "        bad.append((tid, \"missing\"))\n",
    "        continue\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(path)\n",
    "    except Exception as e:\n",
    "        bad.append((tid, str(e)))\n",
    "\n",
    "print(\"Bad files:\", len(bad))\n",
    "bad[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308bcc2-673f-4d69-9c7c-4c3018fc6f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
